---
title: "ERA correlation analysis"
output: html_document
date: "2024-09-16"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Correlation analysis for ERA data set

```{r, warning=FALSE, echo=TRUE, message=FALSE}
# Load packages and custom functions
library("tidymodels")
library("tidyverse")
library("yaml")
source('R/load_and_preprocess.R')
source('R/correlation_analysis.R')
source('R/create_splits.R')
source('R/create_model_spec.R')
source('R/create_recipe.R')
source('R/create_workflow.R')
source('R/tune_model.R')
source('R/train_and_test_final_model.R')
```

## First model with all possible predictors

First, we will run a model with all predictors to see how much variance we could explain by keeping all predictors. This is only for having a reference level, as including progressively more predictors will lead to overfitting. In addition, variable importance will be poorly estimated when many correlated predictors appear in the model.

# Read settings from the config file but overwrite the formula to use all predictors

We should include a measure of centrality (Mean, Median, Mode) for each feature.

```{r }
experiment <- "data_ERA_Clim_Soil"
config <- yaml::read_yaml(paste0("config/", experiment, "_config.yaml"))

### Data settings from config
data_path = config$data_path
target = config$target
general_formula <- as.formula("yield ~ .")
# Preprocessing settings
imputation_method = config$preprocessing$imputation_method
lower_quantile = config$preprocessing$lower_quantile
upper_quantile = config$preprocessing$upper_quantile
# Data splitting settings
train_prop = config$splits$train_prop
n_cv_folds = config$splits$cv_folds
cv_seed = config$splits$seed

### Load and preprocess data
data <- load_and_preprocess(data_path = data_path,
                            target_col = target,
                            cols_to_remove = NULL,
                            imputation_method = imputation_method,
                            lower_quantile = lower_quantile,
                            upper_quantile = upper_quantile)

target_predictors <- colnames(data)[c(18, 128:155, 161:281)]

# Need to remove aliased features

data <- data %>% select(all_of(target_predictors))

print(correlation_analysis(data,
                           target_var = target, 
                           cor_threshold = 0.85))

data <- data %>%
  select(-matches("Mode")) %>%
  select(-matches("Median")) %>%
  select(!any_of(c("GDDmax", "ETo.NA",
                   "Rain.Days.L.1", "Rain.Max.RSeq.1",
                   "Rain.N.RSeq.T1.D7", "Rain.N.RSeq.T1.D14",
                   "Rain.N.RSeq.T1.D21", "Rain.Days.L.1.Dev.Mean",
                   "Rain.Max.RSeq.1.Dev.Mean", "Rain.N.RSeq.T1.D14.Dev.Mean",
                   "Rain.N.RSeq.T1.D7.Dev.Mean",
                   "Rain.N.RSeq.T1.D21.Dev.Mean", "Mean_texture.class",
                   "Rain.Max.RSeq.5", "GDDlow", "GDDopt.Dev.Mean",
                   "Tmean.mean.Dev.Mean", "Mean_mg_mehlich3",
                   "Mean_clay_tot_psa", "Rain.N.RSeq.T0.D21.Dev.Mean",
                   "SD_n_tot_ncs", "Rain.Days.L.5", "Tmean.mean",
                   "Rain.N.RSeq.T5.D21.Dev.Mean", "ETo.sum",
                   "ETo.sum.Dev.Mean", "SD_mg_mehlich3", "Rain.N.RSeq.T5.D21"
                   )))
  
# print(correlation_analysis(data, 
#                            target_var = target, 
#                            cor_threshold = 0.90))

experiment <- "data_ERA_Clim_Soil_exploration"
for (model_i in 2:length(config$models)){
  # The recipe is only standardizing/normalizing predictors ATM
  if (config$models[[model_i]]$recipe_options$transformations) {
    data[[target]] <- log(data[[target]] + 1)
  }
  
  ### Split data
  splits <- create_splits(data,
                          target = target,
                          train_prop = train_prop,
                          cv_folds = n_cv_folds,
                          seed = cv_seed)
  train_test_split <- splits$train_test_split
  train_data <- splits$train_data
  test_data <- splits$test_data
  cv_folds <- splits$cv_folds
  
  
  ### Create recipe
  recipe = create_recipe(data = train_data, 
                         formula = general_formula,
                         one_hot = config$models[[model_i]]$recipe_options$one_hot,
                         standardize = config$models[[model_i]]$recipe_options$standardize,
                         normalize = config$models[[model_i]]$recipe_options$normalize,
                         augment = config$models[[model_i]]$recipe_options$augment,
                         variable_selection = config$models[[model_i]]$recipe_options$variable_selection,
                         parallel = config$models[[model_i]]$recipe_options$parallel)
  
  # Use this code to reverse the standardization/transformation of the data
  prepped_recipe = prep(recipe, training = train_data)
  # See how the data looks after applying the recipe
  # transformed_data = bake(prepped_recipe, new_data = NULL)  # NULL for training data
  
  ### Define tuning parameters
  tuning_params <- setNames(replicate(length(config$models[[model_i]]$tune_params), 
                                      tune(), 
                                      simplify = FALSE), 
                            names(config$models[[model_i]]$tune_params))
  
  ### Create model specification
  model_spec <- create_model_spec(model_type = config$models[[model_i]]$name, 
                                  mode = config$models[[model_i]]$mode,
                                  engine = config$models[[model_i]]$engine,
                                  tune_params = tuning_params,
                                  stop_iter = stop_iter)
  
  ### Create workflow
  wf <- create_workflow(recipe = recipe,
                        model_spec = model_spec)
  

    ### Define the hyperparameter grid
  # Modify code as the code below if the current code fails
  # rf_grid <- expand.grid(
  #   mtry = floor(seq(1, ncol(train_data) - 1, length.out = 5)),
  #   min_n = c(2, 5, 10)
  # )
  if (length(tuning_params) == 1){
  config_grid <- data.frame(
    penalty = as.vector(unlist(config$models[[model_i]]$tune_params)))
  } else {
  config_grid <- do.call(expand.grid, config$models[[model_i]]$tune_params)}

  ### Train and evaluate model
  tune_result <- tune_model(
    workflow = wf,
    resamples = cv_folds,
    grid = config_grid,
    metrics = metric_set(rmse, rsq, mae),
    control = control_grid(save_pred = TRUE),
    verbose = TRUE,
    model_name = config$models[[model_i]]$name,
    output_dir = paste0("model_output/", experiment, "/tuning")
  )
  
  ### Collect the cross-validation metrics
  cv_metrics <- collect_metrics(tune_result)
  print(cv_metrics)
  
  ### Select the best model
  best_model <- select_best(tune_result, 
                            metric = "rsq")
  
  ### Train and test final model
  final_workflow = train_and_test_final_model(workflow = wf, 
                                              target_col = target,
                                              best_model = best_model,
                                              train_data = train_data, 
                                              test_data = test_data, 
                                              model_name = config$models[[model_i]]$name,
                                              output_dir = paste0("model_output/", experiment, "/final_model"))
  test_performance <- read_csv(paste0("model_output/", experiment, "/final_model/", config$models[[model_i]]$name, "_test_metrics.csv"))
  cat(paste0(config$models[[model_i]]$name, " test performance: \n"))
  print(test_performance)
}

```

Now we drop features that do not have a meaningful impact on the model performance.


# Feature selection
A threshold of VIF below 10 or 4 is required for variance partitioning. We are
also excluding features due to scientific knowledge and feature uncertainty.
```{r }
print(correlation_analysis(data,
                           target_var = target, 
                           cor_threshold = 0.85))

data <- data %>%
  select(-matches("mehlich")) %>%
  select(-matches("Mean_ph")) %>%
  select(-matches("GDD")) %>%
  select(-matches("SD_")) %>%
  select(-matches("RSeq")) %>%
  select(!any_of(c("Mean_n_tot_ncs",
                    "Tmean.sd", "Tmax.sd",
                   "Mean_silt_tot_psa", "Rain.sum", "Rain.sum.Dev.Mean",
                   "Mean_bdr", "Mean_db_od"
                   )))

print(correlation_analysis(data,
                           target_var = target, 
                           cor_threshold = 0.85))


```
# See the next fit of the model

```{r}
experiment <- "data_ERA_Clim_Soil_exploration2"
for (model_i in 2:length(config$models)){
  # The recipe is only standardizing/normalizing predictors ATM
  if (config$models[[model_i]]$recipe_options$transformations) {
    data[[target]] <- log(data[[target]] + 1)
  }
  
  ### Split data
  splits <- create_splits(data,
                          target = target,
                          train_prop = train_prop,
                          cv_folds = n_cv_folds,
                          seed = cv_seed)
  train_test_split <- splits$train_test_split
  train_data <- splits$train_data
  test_data <- splits$test_data
  cv_folds <- splits$cv_folds
  
  
  ### Create recipe
  recipe = create_recipe(data = train_data, 
                         formula = general_formula,
                         one_hot = config$models[[model_i]]$recipe_options$one_hot,
                         standardize = config$models[[model_i]]$recipe_options$standardize,
                         normalize = config$models[[model_i]]$recipe_options$normalize,
                         augment = config$models[[model_i]]$recipe_options$augment,
                         variable_selection = config$models[[model_i]]$recipe_options$variable_selection,
                         parallel = config$models[[model_i]]$recipe_options$parallel)
  
  # Use this code to reverse the standardization/transformation of the data
  prepped_recipe = prep(recipe, training = train_data)
  # See how the data looks after applying the recipe
  # transformed_data = bake(prepped_recipe, new_data = NULL)  # NULL for training data
  
  ### Define tuning parameters
  tuning_params <- setNames(replicate(length(config$models[[model_i]]$tune_params), 
                                      tune(), 
                                      simplify = FALSE), 
                            names(config$models[[model_i]]$tune_params))
  
  ### Create model specification
  model_spec <- create_model_spec(model_type = config$models[[model_i]]$name, 
                                  mode = config$models[[model_i]]$mode,
                                  engine = config$models[[model_i]]$engine,
                                  tune_params = tuning_params,
                                  stop_iter = stop_iter)
  
  ### Create workflow
  wf <- create_workflow(recipe = recipe,
                        model_spec = model_spec)
  

    ### Define the hyperparameter grid
  # Modify code as the code below if the current code fails
  # rf_grid <- expand.grid(
  #   mtry = floor(seq(1, ncol(train_data) - 1, length.out = 5)),
  #   min_n = c(2, 5, 10)
  # )
  if (length(tuning_params) == 1){
  config_grid <- data.frame(
    penalty = as.vector(unlist(config$models[[model_i]]$tune_params)))
  } else {
  config_grid <- do.call(expand.grid, config$models[[model_i]]$tune_params)}

  ### Train and evaluate model
  tune_result <- tune_model(
    workflow = wf,
    resamples = cv_folds,
    grid = config_grid,
    metrics = metric_set(rmse, rsq, mae),
    control = control_grid(save_pred = TRUE),
    verbose = TRUE,
    model_name = config$models[[model_i]]$name,
    output_dir = paste0("model_output/", experiment, "/tuning")
  )
  
  ### Collect the cross-validation metrics
  cv_metrics <- collect_metrics(tune_result)
  print(cv_metrics)
  
  ### Select the best model
  best_model <- select_best(tune_result, 
                            metric = "rsq")
  
  ### Train and test final model
  final_workflow = train_and_test_final_model(workflow = wf, 
                                              target_col = target,
                                              best_model = best_model,
                                              train_data = train_data, 
                                              test_data = test_data, 
                                              model_name = config$models[[model_i]]$name,
                                              output_dir = paste0("model_output/", experiment, "/final_model"))
  test_performance <- read_csv(paste0("model_output/", experiment, "/final_model/", config$models[[model_i]]$name, "_test_metrics.csv"))
  cat(paste0(config$models[[model_i]]$name, " test performance: \n"))
  print(test_performance)
}

```



